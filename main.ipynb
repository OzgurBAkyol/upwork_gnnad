{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:12:04.776346Z",
     "start_time": "2024-10-06T17:12:03.215446Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:03.472119Z",
     "start_time": "2024-10-06T17:37:12.748878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Veriyi birleştirme ve ön işleme\n",
    "file_path = './Datasets/*.csv'\n",
    "all_files = glob.glob(file_path)\n",
    "df_list = []\n",
    "\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list)\n",
    "combined_df.sort_values(by='timestamp', inplace=True)\n",
    "output_file = 'combined.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "data_k = pd.read_csv('combined.csv')"
   ],
   "id": "fe3c5329a36ff71",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:45.689597Z",
     "start_time": "2024-10-06T17:38:39.791346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sadece gerekli kolonları seçelim\n",
    "data_k_filtered = data_k[['timestamp', 'sensor', 'pm25_avg_60']]\n",
    "data_k_filtered['timestamp'] = pd.to_datetime(data_k_filtered['timestamp'])\n",
    "\n",
    "# Saatlik ortalama\n",
    "hourly_avg = data_k_filtered.resample('H', on='timestamp').mean().reset_index()\n",
    "\n",
    "# Rüzgar verisi ekleme\n",
    "wind = pd.read_csv('fresno_wind.csv')\n",
    "wind['datetime'] = pd.to_datetime(wind['datetime']).dt.tz_localize('UTC')\n",
    "\n",
    "# Verileri birleştirme\n",
    "merged_data = pd.merge(hourly_avg, wind[['datetime', 'windspeed', 'winddir']], left_on='timestamp', right_on='datetime',\n",
    "                       how='inner')\n",
    "merged_data = merged_data.drop(columns=['datetime'])"
   ],
   "id": "4c4eadc20e13841c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2170410342.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_k_filtered['timestamp'] = pd.to_datetime(data_k_filtered['timestamp'])\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2170410342.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  hourly_avg = data_k_filtered.resample('H', on='timestamp').mean().reset_index()\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:48.438202Z",
     "start_time": "2024-10-06T17:38:48.430846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Veriyi normalleştirme fonksiyonu\n",
    "def normalise(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns)"
   ],
   "id": "ac153c2074eeca67",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:49.777473Z",
     "start_time": "2024-10-06T17:38:49.743721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Özellikleri normalleştir\n",
    "features = ['pm25_avg_60', 'windspeed', 'winddir']\n",
    "merged_data[features] = normalise(merged_data[features])"
   ],
   "id": "8c31767d75961b67",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:26:52.843100Z",
     "start_time": "2024-10-06T18:26:52.777282Z"
    }
   },
   "cell_type": "code",
   "source": "merged_data.info()",
   "id": "18daa80a68b49081",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2924 entries, 0 to 2923\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   timestamp    2924 non-null   datetime64[ns, UTC]\n",
      " 1   pm25_avg_60  2924 non-null   float64            \n",
      " 2   windspeed    2924 non-null   float64            \n",
      " 3   winddir      2924 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(3)\n",
      "memory usage: 114.2 KB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:53.444692Z",
     "start_time": "2024-10-06T17:38:53.416568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Anomali üretim sınıfı\n",
    "class AnomalyGenerator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def generate_fgsm(self, data, eps=0.1):\n",
    "        # FGSM kullanarak anomaliler oluştur\n",
    "        adv_data = data.clone().detach()\n",
    "        noise = eps * torch.sign(torch.randn_like(data))  # Rastgele gürültü ekle\n",
    "        adv_data += noise\n",
    "        adv_data = torch.clamp(adv_data, 0, 1)  # 0 ve 1 arasında kısıtla\n",
    "        return adv_data\n",
    "\n",
    "    def generate_bim(self, data, eps=0.1, alpha=0.01, steps=10):\n",
    "        # BIM kullanarak anomaliler oluştur\n",
    "        adv_data = data.clone().detach()\n",
    "        for _ in range(steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            adv_data += alpha * adv_data.grad.sign()\n",
    "            adv_data = torch.clamp(adv_data, 0, 1)  # 0 ve 1 arasında kısıtla\n",
    "            adv_data.grad.zero_()\n",
    "        return adv_data\n",
    "\n",
    "    def generate_pgd(self, data, eps=0.1, alpha=0.01, steps=10):\n",
    "        # PGD kullanarak anomaliler oluştur\n",
    "        adv_data = data.clone().detach()\n",
    "        for _ in range(steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            adv_data = adv_data.detach() + alpha * adv_data.grad.sign()\n",
    "            delta = torch.clamp(adv_data - data, min=-eps, max=eps)\n",
    "            adv_data = torch.clamp(data + delta, min=0, max=1).detach()\n",
    "            adv_data.grad.zero_()\n",
    "        return adv_data"
   ],
   "id": "4c5210960216913d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:55.323874Z",
     "start_time": "2024-10-06T17:38:55.305035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Veriyi 28:4:9 oranında ayırma\n",
    "train_size = 0.28\n",
    "val_size = 0.04\n",
    "test_size = 0.09\n",
    "\n",
    "# İlk olarak, train setini ayırıyoruz.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, train_size=train_size, random_state=42)\n",
    "\n",
    "# Daha sonra kalan veriyi validation ve test seti olarak ayırıyoruz.\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size/(test_size + val_size), random_state=42)"
   ],
   "id": "40df20d19385a9f9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:58.071330Z",
     "start_time": "2024-10-06T17:38:57.199859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# GraPhy katmanı tanımlama\n",
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# GraPhy Modeli\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=512, output_size=2):\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layer1 = GraPhyLayer(input_size, hidden_dim)\n",
    "        self.layer2 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer5 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Modeli oluşturma\n",
    "input_size = X_train.shape[1]\n",
    "model = GraPhyModel(input_size)\n",
    "\n",
    "# Adam optimizer'ı ve parametreleri\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "# Kayıp fonksiyonu\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "e916fbeb764484d8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:39:09.337064Z",
     "start_time": "2024-10-06T17:39:01.086658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Tensor Dataset ve DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Her epoch sonunda doğrulama seti ile değerlendirme\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ],
   "id": "dc77e1ba79b33cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 12.2682, Validation Loss: 4.4748\n",
      "Epoch [2/50], Loss: 6.0266, Validation Loss: 4.3963\n",
      "Epoch [3/50], Loss: 5.5184, Validation Loss: 4.3354\n",
      "Epoch [4/50], Loss: 5.5581, Validation Loss: 4.2708\n",
      "Epoch [5/50], Loss: 5.3445, Validation Loss: 4.2451\n",
      "Epoch [6/50], Loss: 5.3636, Validation Loss: 4.2887\n",
      "Epoch [7/50], Loss: 5.2052, Validation Loss: 4.2075\n",
      "Epoch [8/50], Loss: 5.2350, Validation Loss: 4.1990\n",
      "Epoch [9/50], Loss: 5.3892, Validation Loss: 4.1421\n",
      "Epoch [10/50], Loss: 5.2214, Validation Loss: 4.1043\n",
      "Epoch [11/50], Loss: 5.1814, Validation Loss: 4.1141\n",
      "Epoch [12/50], Loss: 5.1718, Validation Loss: 4.1037\n",
      "Epoch [13/50], Loss: 5.1611, Validation Loss: 4.1176\n",
      "Epoch [14/50], Loss: 5.2026, Validation Loss: 4.1623\n",
      "Epoch [15/50], Loss: 5.1363, Validation Loss: 4.0717\n",
      "Epoch [16/50], Loss: 5.0201, Validation Loss: 4.0594\n",
      "Epoch [17/50], Loss: 5.1317, Validation Loss: 4.0794\n",
      "Epoch [18/50], Loss: 5.0604, Validation Loss: 4.0745\n",
      "Epoch [19/50], Loss: 5.1062, Validation Loss: 4.1288\n",
      "Epoch [20/50], Loss: 5.0869, Validation Loss: 4.1300\n",
      "Epoch [21/50], Loss: 5.1495, Validation Loss: 4.0701\n",
      "Epoch [22/50], Loss: 5.0598, Validation Loss: 4.0970\n",
      "Epoch [23/50], Loss: 4.9531, Validation Loss: 4.0676\n",
      "Epoch [24/50], Loss: 4.9620, Validation Loss: 4.0796\n",
      "Epoch [25/50], Loss: 5.0006, Validation Loss: 4.1212\n",
      "Epoch [26/50], Loss: 4.9860, Validation Loss: 4.1088\n",
      "Epoch [27/50], Loss: 4.9378, Validation Loss: 4.0918\n",
      "Epoch [28/50], Loss: 4.9782, Validation Loss: 4.2050\n",
      "Epoch [29/50], Loss: 4.9527, Validation Loss: 4.1077\n",
      "Epoch [30/50], Loss: 5.0371, Validation Loss: 4.1642\n",
      "Epoch [31/50], Loss: 4.9297, Validation Loss: 4.1354\n",
      "Epoch [32/50], Loss: 4.9677, Validation Loss: 4.1473\n",
      "Epoch [33/50], Loss: 4.8663, Validation Loss: 4.2289\n",
      "Epoch [34/50], Loss: 4.8612, Validation Loss: 4.1883\n",
      "Epoch [35/50], Loss: 5.0238, Validation Loss: 4.2541\n",
      "Epoch [36/50], Loss: 4.9348, Validation Loss: 4.2655\n",
      "Epoch [37/50], Loss: 4.9263, Validation Loss: 4.2051\n",
      "Epoch [38/50], Loss: 4.8629, Validation Loss: 4.2542\n",
      "Epoch [39/50], Loss: 4.7346, Validation Loss: 4.2352\n",
      "Epoch [40/50], Loss: 4.8133, Validation Loss: 4.2444\n",
      "Epoch [41/50], Loss: 4.7340, Validation Loss: 4.3522\n",
      "Epoch [42/50], Loss: 4.7529, Validation Loss: 4.3358\n",
      "Epoch [43/50], Loss: 4.8819, Validation Loss: 4.3905\n",
      "Epoch [44/50], Loss: 4.8636, Validation Loss: 4.4367\n",
      "Epoch [45/50], Loss: 4.6463, Validation Loss: 4.4245\n",
      "Epoch [46/50], Loss: 4.7224, Validation Loss: 4.4387\n",
      "Epoch [47/50], Loss: 4.5840, Validation Loss: 4.6326\n",
      "Epoch [48/50], Loss: 4.6791, Validation Loss: 4.5598\n",
      "Epoch [49/50], Loss: 4.6673, Validation Loss: 4.4985\n",
      "Epoch [50/50], Loss: 4.6957, Validation Loss: 4.5808\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:39:12.708215Z",
     "start_time": "2024-10-06T17:39:12.649684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test setinde modeli değerlendirme\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ],
   "id": "5307c07d22ee65aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.79%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:41:27.631729Z",
     "start_time": "2024-10-06T17:41:27.307343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Modeli test setinde değerlendirme\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Precision, Recall, F1-Score ve Accuracy hesaplama\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "report = classification_report(all_labels, all_predictions, target_names=['Normal', 'Anomali'])\n",
    "\n",
    "# Confusion Matrix ile specificity hesaplama\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "specificity = tn / (tn + fp)  # Özgüllük (Specificity)\n",
    "precision = tp / (tp + fp)    # Doğruluk (Precision)\n",
    "recall = tp / (tp + fn)       # Duyarlılık (Recall)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)  # F1 Skoru\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "id": "9f4c682b46f37369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      1.00      0.97      1382\n",
      "     Anomali       0.00      0.00      0.00        76\n",
      "\n",
      "    accuracy                           0.95      1458\n",
      "   macro avg       0.47      0.50      0.49      1458\n",
      "weighted avg       0.90      0.95      0.92      1458\n",
      "\n",
      "Accuracy: 0.9479\n",
      "Specificity: 1.0000\n",
      "Precision: nan\n",
      "Recall: 0.0000\n",
      "F1 Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2135518426.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)    # Doğruluk (Precision)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Anomali yaratma sınıfı\n",
    "class AnomalyGenerator:\n",
    "    def __init__(self, model, eps=0.1, alpha=0.01, steps=10, random_start=True):\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.random_start = random_start\n",
    "\n",
    "    def fgsm(self, data, labels):\n",
    "        data = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        grad = torch.autograd.grad(loss, data)[0]\n",
    "        adv_data = data + self.eps * grad.sign()\n",
    "        return torch.clamp(adv_data, min=0, max=1)\n",
    "\n",
    "    def generate_anomalies(self, data, labels, method='fgsm'):\n",
    "        if method == 'fgsm':\n",
    "            return self.fgsm(data, labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")"
   ],
   "id": "a3f0cb5ad98d16a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Anomaliler üretme (isteğe bağlı)\n",
    "anomaly_gen = AnomalyGenerator(model)\n",
    "X_test_adv = anomaly_gen.generate_anomalies(X_test, y_test)"
   ],
   "id": "7d695d65fbaaa289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "814b5163cc14cf1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32bca0d31763aa7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:33.812624Z",
     "start_time": "2024-10-06T18:34:33.775598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Veri setini yükle\n",
    "# merged_data = pd.read_csv('your_data.csv') # Veriyi yükleyin\n",
    "# Özellikleri normalleştir\n",
    "def normalise(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "features = ['pm25_avg_60', 'windspeed', 'winddir']\n",
    "merged_data[features] = normalise(merged_data[features])\n",
    "\n",
    "# Eğitim, doğrulama ve test setlerine ayır\n",
    "X = merged_data[features].values\n",
    "y = np.zeros(merged_data.shape[0], dtype=int)\n",
    "num_anomalies = int(0.05 * merged_data.shape[0])\n",
    "anomaly_indices = np.random.choice(merged_data.index, num_anomalies, replace=False)\n",
    "y[anomaly_indices] = 1  # 1: Anomali\n",
    "\n",
    "# Veriyi eğitim, doğrulama ve test setlerine ayır\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.13, random_state=42)  # %28 eğitim\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6923, random_state=42)  # %4 doğrulama ve %9 test"
   ],
   "id": "83ea84922a63b00d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:35.856854Z",
     "start_time": "2024-10-06T18:34:35.851098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GraPhy katmanı tanımlama\n",
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# GraPhy Modeli\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=512, output_size=2):\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layer1 = GraPhyLayer(input_size, hidden_dim)\n",
    "        self.layer2 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer5 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ],
   "id": "ebf36b8ecde56a87",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:36.879673Z",
     "start_time": "2024-10-06T18:34:36.862700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnomalyGenerator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def generate_fgsm(self, data, labels, eps=0.1):\n",
    "        data = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # FGSM ile anomali üret\n",
    "        adv_data = data + eps * data.grad.sign()\n",
    "        return adv_data.detach()\n",
    "\n",
    "    def generate_bim(self, data, labels, eps=0.1, alpha=0.01, steps=10):\n",
    "        data = data.clone().detach()\n",
    "        ori_data = data.clone().detach()\n",
    "\n",
    "        for _ in range(steps):\n",
    "            data.requires_grad = True\n",
    "            outputs = self.model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            data = data + alpha * data.grad.sign()\n",
    "            data = torch.clamp(data, ori_data - eps, ori_data + eps).detach()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def generate_pgd(self, data, labels, eps=0.1, alpha=0.01, steps=10, random_start=True):\n",
    "        data = data.clone().detach()\n",
    "        if random_start:\n",
    "            data = data + torch.empty_like(data).uniform_(-eps, eps)\n",
    "            data = torch.clamp(data, min=0, max=1).detach()\n",
    "\n",
    "        ori_data = data.clone().detach()\n",
    "\n",
    "        for _ in range(steps):\n",
    "            data.requires_grad = True\n",
    "            outputs = self.model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            data = data + alpha * data.grad.sign()\n",
    "            data = torch.clamp(data, ori_data - eps, ori_data + eps).detach()\n",
    "\n",
    "        return data"
   ],
   "id": "1cdd43a8a6a2de76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:35:05.185383Z",
     "start_time": "2024-10-06T18:34:37.661164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modeli oluşturma\n",
    "input_size = X_train.shape[1]\n",
    "model = GraPhyModel(input_size)\n",
    "\n",
    "# Adam optimizer'ı ve kayıp fonksiyonu\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tensor Dataset ve DataLoader\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Her epoch sonunda doğrulama seti ile değerlendirme\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ],
   "id": "226a3b8d48086846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 24.1026, Validation Loss: 1.5193\n",
      "Epoch [2/50], Loss: 15.6634, Validation Loss: 1.3401\n",
      "Epoch [3/50], Loss: 15.5176, Validation Loss: 1.4785\n",
      "Epoch [4/50], Loss: 15.0240, Validation Loss: 1.3777\n",
      "Epoch [5/50], Loss: 14.8722, Validation Loss: 1.3524\n",
      "Epoch [6/50], Loss: 14.8079, Validation Loss: 1.2901\n",
      "Epoch [7/50], Loss: 14.6921, Validation Loss: 1.3838\n",
      "Epoch [8/50], Loss: 14.7178, Validation Loss: 1.4183\n",
      "Epoch [9/50], Loss: 14.8644, Validation Loss: 1.3729\n",
      "Epoch [10/50], Loss: 14.6545, Validation Loss: 1.3491\n",
      "Epoch [11/50], Loss: 14.7580, Validation Loss: 1.3560\n",
      "Epoch [12/50], Loss: 14.6857, Validation Loss: 1.4932\n",
      "Epoch [13/50], Loss: 14.6414, Validation Loss: 1.4618\n",
      "Epoch [14/50], Loss: 14.5706, Validation Loss: 1.4367\n",
      "Epoch [15/50], Loss: 14.4608, Validation Loss: 1.3983\n",
      "Epoch [16/50], Loss: 14.4717, Validation Loss: 1.4505\n",
      "Epoch [17/50], Loss: 14.5682, Validation Loss: 1.4234\n",
      "Epoch [18/50], Loss: 14.3284, Validation Loss: 1.5200\n",
      "Epoch [19/50], Loss: 14.4042, Validation Loss: 1.4461\n",
      "Epoch [20/50], Loss: 14.4444, Validation Loss: 1.4023\n",
      "Epoch [21/50], Loss: 14.3007, Validation Loss: 1.4395\n",
      "Epoch [22/50], Loss: 14.2683, Validation Loss: 1.4379\n",
      "Epoch [23/50], Loss: 14.2020, Validation Loss: 1.4636\n",
      "Epoch [24/50], Loss: 14.4028, Validation Loss: 1.3989\n",
      "Epoch [25/50], Loss: 14.2638, Validation Loss: 1.4179\n",
      "Epoch [26/50], Loss: 14.2891, Validation Loss: 1.5786\n",
      "Epoch [27/50], Loss: 14.0325, Validation Loss: 1.6681\n",
      "Epoch [28/50], Loss: 14.2216, Validation Loss: 1.4331\n",
      "Epoch [29/50], Loss: 14.4053, Validation Loss: 1.4962\n",
      "Epoch [30/50], Loss: 14.3386, Validation Loss: 1.3423\n",
      "Epoch [31/50], Loss: 14.1795, Validation Loss: 1.4994\n",
      "Epoch [32/50], Loss: 14.2345, Validation Loss: 1.4586\n",
      "Epoch [33/50], Loss: 13.9529, Validation Loss: 1.5145\n",
      "Epoch [34/50], Loss: 14.1309, Validation Loss: 1.5285\n",
      "Epoch [35/50], Loss: 14.1108, Validation Loss: 1.4103\n",
      "Epoch [36/50], Loss: 14.2714, Validation Loss: 1.5869\n",
      "Epoch [37/50], Loss: 14.1025, Validation Loss: 1.3617\n",
      "Epoch [38/50], Loss: 13.9786, Validation Loss: 1.6674\n",
      "Epoch [39/50], Loss: 14.1396, Validation Loss: 1.7348\n",
      "Epoch [40/50], Loss: 14.4306, Validation Loss: 1.4937\n",
      "Epoch [41/50], Loss: 14.0409, Validation Loss: 1.4048\n",
      "Epoch [42/50], Loss: 13.9067, Validation Loss: 1.5212\n",
      "Epoch [43/50], Loss: 14.0531, Validation Loss: 1.4908\n",
      "Epoch [44/50], Loss: 13.9232, Validation Loss: 1.4305\n",
      "Epoch [45/50], Loss: 13.9699, Validation Loss: 1.4469\n",
      "Epoch [46/50], Loss: 13.9293, Validation Loss: 1.4569\n",
      "Epoch [47/50], Loss: 14.1389, Validation Loss: 1.4516\n",
      "Epoch [48/50], Loss: 14.0217, Validation Loss: 1.4855\n",
      "Epoch [49/50], Loss: 13.8455, Validation Loss: 1.6031\n",
      "Epoch [50/50], Loss: 13.7397, Validation Loss: 1.5136\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:35:10.908828Z",
     "start_time": "2024-10-06T18:35:08.975051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Anomali verilerini üret\n",
    "anomaly_generator = AnomalyGenerator(model)\n",
    "\n",
    "# FGSM ile anomali üret\n",
    "fgsm_anomalies = anomaly_generator.generate_fgsm(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "\n",
    "# BIM ile anomali üret\n",
    "bim_anomalies = anomaly_generator.generate_bim(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "\n",
    "# PGD ile anomali üret\n",
    "pgd_anomalies = anomaly_generator.generate_pgd(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "\n",
    "# Anomali verilerini test setine ekleyin\n",
    "X_test_with_anomalies = np.vstack((X_test, fgsm_anomalies.numpy(), bim_anomalies.numpy(), pgd_anomalies.numpy()))\n",
    "y_test_with_anomalies = np.hstack((y_test, np.ones(fgsm_anomalies.shape[0] + bim_anomalies.shape[0] + pgd_anomalies.shape[0])))\n",
    "\n",
    "# Test setini oluştur\n",
    "X_test_tensor = torch.FloatTensor(X_test_with_anomalies)\n",
    "y_test_tensor = torch.LongTensor(y_test_with_anomalies)\n",
    "\n",
    "# Modelin test edilmesi\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "# Sonuçları değerlendir\n",
    "accuracy = accuracy_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "precision = precision_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "recall = recall_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_tensor.numpy(), predicted.numpy()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Metrikleri yazdır\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Specificity: {specificity:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ],
   "id": "7708e4cacc0f25ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0310\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Specificity: 1.0000\n",
      "F1 Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2242473391.py:33: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f1 = 2 * (precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:07:56.884565Z",
     "start_time": "2024-10-06T19:07:56.280767Z"
    }
   },
   "cell_type": "code",
   "source": "merged_data.info()",
   "id": "fdb960878cb7019",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2924 entries, 0 to 2923\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   timestamp    2924 non-null   datetime64[ns, UTC]\n",
      " 1   pm25_avg_60  2924 non-null   float64            \n",
      " 2   windspeed    2924 non-null   float64            \n",
      " 3   winddir      2924 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(3)\n",
      "memory usage: 178.8 KB\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:38:57.948572Z",
     "start_time": "2024-10-06T18:38:57.917683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Veri setini yükle\n",
    "# merged_data = pd.read_csv('your_data.csv') # Veriyi yükleyin\n",
    "\n",
    "# Özellikleri normalleştir\n",
    "def normalise(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "features = ['pm25_avg_60', 'windspeed', 'winddir']\n",
    "merged_data[features] = normalise(merged_data[features])"
   ],
   "id": "da745c54ae8fa3aa",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:38:58.738584Z",
     "start_time": "2024-10-06T18:38:58.717196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Anomali verilerini üret\n",
    "def generate_anomalies(data, num_anomalies=1000):\n",
    "    anomalies = []\n",
    "    for _ in range(num_anomalies):\n",
    "        index = np.random.randint(0, data.shape[0])\n",
    "        anomaly = data[index] + np.random.normal(0, 1, data[index].shape)  # Normal dağılımdan gürültü ekleyerek anomali üret\n",
    "        anomalies.append(anomaly)\n",
    "    return np.array(anomalies)\n",
    "\n",
    "# Normal verileri ayır\n",
    "normal_data = merged_data[features].values\n",
    "\n",
    "# Anomalileri üret\n",
    "anomaly_data = generate_anomalies(normal_data, num_anomalies=1000)  # 1000 anomali üret"
   ],
   "id": "39ae7150ac0ec6c4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:39:43.067343Z",
     "start_time": "2024-10-06T18:39:43.045990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.vstack((normal_data, anomaly_data))\n",
    "y = np.hstack((np.zeros(normal_data.shape[0]), np.ones(anomaly_data.shape[0])))  # 0: Normal, 1: Anomali\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.13, random_state=42)  # %28 eğitim\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6923, random_state=42)  # %4 doğrulama ve %9 test"
   ],
   "id": "315ec490fc274f46",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:39:47.641090Z",
     "start_time": "2024-10-06T18:39:47.632525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=512, output_size=2):\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layer1 = GraPhyLayer(input_size, hidden_dim)\n",
    "        self.layer2 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer5 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ],
   "id": "ebe56545f55b7765",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:40:15.936946Z",
     "start_time": "2024-10-06T18:39:51.707705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = GraPhyModel(input_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ],
   "id": "671b45462bc52ff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 60.1515, Validation Loss: 2.5585\n",
      "Epoch [2/50], Loss: 44.4334, Validation Loss: 2.1748\n",
      "Epoch [3/50], Loss: 37.2413, Validation Loss: 1.8297\n",
      "Epoch [4/50], Loss: 34.1474, Validation Loss: 1.7889\n",
      "Epoch [5/50], Loss: 31.9695, Validation Loss: 1.6694\n",
      "Epoch [6/50], Loss: 30.1265, Validation Loss: 1.6336\n",
      "Epoch [7/50], Loss: 29.5877, Validation Loss: 1.5462\n",
      "Epoch [8/50], Loss: 29.4214, Validation Loss: 1.4780\n",
      "Epoch [9/50], Loss: 28.1360, Validation Loss: 1.6396\n",
      "Epoch [10/50], Loss: 28.2332, Validation Loss: 1.5894\n",
      "Epoch [11/50], Loss: 27.0595, Validation Loss: 1.4489\n",
      "Epoch [12/50], Loss: 26.7501, Validation Loss: 1.5674\n",
      "Epoch [13/50], Loss: 26.2023, Validation Loss: 1.5061\n",
      "Epoch [14/50], Loss: 26.1530, Validation Loss: 1.5522\n",
      "Epoch [15/50], Loss: 25.7509, Validation Loss: 1.5736\n",
      "Epoch [16/50], Loss: 26.2591, Validation Loss: 1.4641\n",
      "Epoch [17/50], Loss: 25.2213, Validation Loss: 1.4762\n",
      "Epoch [18/50], Loss: 24.6848, Validation Loss: 1.4933\n",
      "Epoch [19/50], Loss: 24.2464, Validation Loss: 1.4335\n",
      "Epoch [20/50], Loss: 24.3083, Validation Loss: 1.4593\n",
      "Epoch [21/50], Loss: 24.5605, Validation Loss: 1.4317\n",
      "Epoch [22/50], Loss: 23.3696, Validation Loss: 1.4468\n",
      "Epoch [23/50], Loss: 23.6004, Validation Loss: 1.5441\n",
      "Epoch [24/50], Loss: 23.3293, Validation Loss: 1.4660\n",
      "Epoch [25/50], Loss: 22.8940, Validation Loss: 1.4556\n",
      "Epoch [26/50], Loss: 23.0645, Validation Loss: 1.5473\n",
      "Epoch [27/50], Loss: 23.0796, Validation Loss: 1.4333\n",
      "Epoch [28/50], Loss: 22.0331, Validation Loss: 1.4484\n",
      "Epoch [29/50], Loss: 22.2087, Validation Loss: 1.3129\n",
      "Epoch [30/50], Loss: 21.5647, Validation Loss: 1.4631\n",
      "Epoch [31/50], Loss: 21.6691, Validation Loss: 1.4889\n",
      "Epoch [32/50], Loss: 22.0339, Validation Loss: 1.5157\n",
      "Epoch [33/50], Loss: 21.0057, Validation Loss: 1.3381\n",
      "Epoch [34/50], Loss: 20.3418, Validation Loss: 1.4100\n",
      "Epoch [35/50], Loss: 21.3015, Validation Loss: 1.2924\n",
      "Epoch [36/50], Loss: 20.4548, Validation Loss: 1.5256\n",
      "Epoch [37/50], Loss: 20.2126, Validation Loss: 1.4036\n",
      "Epoch [38/50], Loss: 19.8283, Validation Loss: 1.3927\n",
      "Epoch [39/50], Loss: 19.5497, Validation Loss: 1.4366\n",
      "Epoch [40/50], Loss: 20.7876, Validation Loss: 1.3551\n",
      "Epoch [41/50], Loss: 19.8916, Validation Loss: 1.3751\n",
      "Epoch [42/50], Loss: 19.7614, Validation Loss: 1.3934\n",
      "Epoch [43/50], Loss: 21.0793, Validation Loss: 1.4120\n",
      "Epoch [44/50], Loss: 19.5036, Validation Loss: 1.3156\n",
      "Epoch [45/50], Loss: 19.7882, Validation Loss: 1.4077\n",
      "Epoch [46/50], Loss: 19.8729, Validation Loss: 1.4357\n",
      "Epoch [47/50], Loss: 18.7656, Validation Loss: 1.3462\n",
      "Epoch [48/50], Loss: 19.4130, Validation Loss: 1.4644\n",
      "Epoch [49/50], Loss: 21.4581, Validation Loss: 1.3262\n",
      "Epoch [50/50], Loss: 18.9836, Validation Loss: 1.3996\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:40:19.463004Z",
     "start_time": "2024-10-06T18:40:19.449657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.FloatTensor(X_test))\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "precision = precision_score(y_test, predicted.numpy())\n",
    "recall = recall_score(y_test, predicted.numpy())\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted.numpy()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Specificity: {specificity:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ],
   "id": "3f8ec538f8f7250f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8898\n",
      "Precision: 0.8684\n",
      "Recall: 0.6947\n",
      "Specificity: 0.9614\n",
      "F1 Score: 0.7719\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:41:42.629705Z",
     "start_time": "2024-10-06T18:41:42.598376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test_outputs = model(test_inputs)\n",
    "\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "total_anomalies = np.sum(y_test)  \n",
    "correct_predictions = np.sum(predicted.numpy()[y_test == 1])  # Modelin doğru tahmin ettiği anomaliler\n",
    "\n",
    "print(f\"Toplam gerçek anomali sayısı: {total_anomalies}\")\n",
    "print(f\"Modelin doğru tahmin ettiği anomali sayısı: {correct_predictions}\")\n",
    "print(f\"Modelin doğruluk oranı: {correct_predictions / total_anomalies * 100:.2f}%\")"
   ],
   "id": "1c246e3798f4bcea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam gerçek anomali sayısı: 95.0\n",
      "Modelin doğru tahmin ettiği anomali sayısı: 66\n",
      "Modelin doğruluk oranı: 69.47%\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66b69555309c0e54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfacf7241a7c6f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d889fc5ba5fcf690"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
