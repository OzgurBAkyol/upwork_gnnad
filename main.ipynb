{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:12:04.776346Z",
     "start_time": "2024-10-06T17:12:03.215446Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:03.472119Z",
     "start_time": "2024-10-06T17:37:12.748878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def combine_and_save_csv(input_path, output_file, sort_column):\n",
    "    all_files = glob.glob(input_path)\n",
    "    df_list = [pd.read_csv(file) for file in all_files]\n",
    "\n",
    "    combined_df = pd.concat(df_list)\n",
    "    combined_df.sort_values(by=sort_column, inplace=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Kullanım\n",
    "file_path = './Datasets/*.csv'\n",
    "output_file = 'combined.csv'\n",
    "sort_column = 'timestamp'\n",
    "\n",
    "data_k = combine_and_save_csv(file_path, output_file, sort_column)"
   ],
   "id": "fe3c5329a36ff71",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:45.689597Z",
     "start_time": "2024-10-06T17:38:39.791346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_and_merge_data(data, wind_file, timestamp_col, resample_freq, merge_cols, drop_cols):\n",
    "    \"\"\"\n",
    "    Veriyi işleyip, rüzgar verisiyle birleştirir.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): İşlenecek veri çerçevesi.\n",
    "        wind_file (str): Rüzgar verisinin dosya yolu.\n",
    "        timestamp_col (str): Zaman damgası sütununun adı.\n",
    "        resample_freq (str): Yeniden örnekleme frekansı (ör. 'H' - saatlik).\n",
    "        merge_cols (list): Rüzgar verisinden birleştirilecek sütunlar.\n",
    "        drop_cols (list): Birleştirme sonrası kaldırılacak sütunlar.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: İşlenmiş ve birleştirilmiş veri çerçevesi.\n",
    "    \"\"\"\n",
    "    # Zaman damgasını datetime formatına çevir\n",
    "    data[timestamp_col] = pd.to_datetime(data[timestamp_col])\n",
    "\n",
    "    # Yeniden örnekleme\n",
    "    resampled_data = data.resample(resample_freq, on=timestamp_col).mean().reset_index()\n",
    "\n",
    "    # Rüzgar verisini yükle\n",
    "    wind = pd.read_csv(wind_file)\n",
    "    wind['datetime'] = pd.to_datetime(wind['datetime']).dt.tz_localize('UTC')\n",
    "\n",
    "    # Verileri birleştir\n",
    "    merged_data = pd.merge(resampled_data, wind[merge_cols], left_on=timestamp_col, right_on='datetime', how='inner')\n",
    "\n",
    "    # Belirtilen sütunları kaldır\n",
    "    merged_data = merged_data.drop(columns=drop_cols)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Kullanım\n",
    "data_k_filtered = data_k[['timestamp', 'sensor', 'pm25_avg_60']]\n",
    "wind_file = 'fresno_wind.csv'\n",
    "timestamp_col = 'timestamp'\n",
    "resample_freq = 'H'\n",
    "merge_cols = ['datetime', 'windspeed', 'winddir']\n",
    "drop_cols = ['datetime']\n",
    "\n",
    "merged_data = process_and_merge_data(data_k_filtered, wind_file, timestamp_col, resample_freq, merge_cols, drop_cols)"
   ],
   "id": "4c4eadc20e13841c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2170410342.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_k_filtered['timestamp'] = pd.to_datetime(data_k_filtered['timestamp'])\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2170410342.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  hourly_avg = data_k_filtered.resample('H', on='timestamp').mean().reset_index()\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:48.438202Z",
     "start_time": "2024-10-06T17:38:48.430846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def normalise(X, scaler=None, columns=None):\n",
    "    \"\"\"\n",
    "    Veriyi ölçeklendirmek için dinamik bir fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Ölçeklenecek veri çerçevesi.\n",
    "        scaler (object): Kullanılacak ölçekleyici (ör. StandardScaler, MinMaxScaler, RobustScaler).\n",
    "        columns (list): Ölçeklenecek sütunların adları. None ise tüm sütunlar ölçeklenir.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Ölçeklenmiş veri çerçevesi.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()  # Varsayılan olarak StandardScaler kullanılır\n",
    "\n",
    "    if columns is None:\n",
    "        columns = X.columns  # Tüm sütunlar ölçeklenir\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X[columns])\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=columns, index=X.index)\n",
    "\n",
    "    # Ölçeklenmeyen sütunları koru\n",
    "    for col in X.columns:\n",
    "        if col not in columns:\n",
    "            X_scaled_df[col] = X[col]\n",
    "\n",
    "    return X_scaled_df\n",
    "\n",
    "# Kullanım\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = normalise(merged_data, scaler=scaler, columns=['pm25_avg_60', 'windspeed', 'winddir'])"
   ],
   "id": "ac153c2074eeca67",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:49.777473Z",
     "start_time": "2024-10-06T17:38:49.743721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = ['pm25_avg_60', 'windspeed', 'winddir']\n",
    "merged_data[features] = normalise(merged_data[features])"
   ],
   "id": "8c31767d75961b67",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:26:52.843100Z",
     "start_time": "2024-10-06T18:26:52.777282Z"
    }
   },
   "cell_type": "code",
   "source": "merged_data.info()",
   "id": "18daa80a68b49081",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2924 entries, 0 to 2923\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   timestamp    2924 non-null   datetime64[ns, UTC]\n",
      " 1   pm25_avg_60  2924 non-null   float64            \n",
      " 2   windspeed    2924 non-null   float64            \n",
      " 3   winddir      2924 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(3)\n",
      "memory usage: 114.2 KB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:53.444692Z",
     "start_time": "2024-10-06T17:38:53.416568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AnomalyGenerator:\n",
    "    def __init__(self, model, eps=0.1, alpha=0.01, steps=10):\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "\n",
    "    def generate_fgsm(self, data, labels):\n",
    "        data = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        adv_data = data + self.eps * data.grad.sign()\n",
    "        return torch.clamp(adv_data, 0, 1)\n",
    "\n",
    "    def generate_bim(self, data, labels):\n",
    "        adv_data = data.clone().detach()\n",
    "        for _ in range(self.steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            adv_data = adv_data + self.alpha * adv_data.grad.sign()\n",
    "            adv_data = torch.clamp(adv_data, 0, 1).detach()\n",
    "        return adv_data\n",
    "\n",
    "    def generate_pgd(self, data, labels, random_start=True):\n",
    "        adv_data = data.clone().detach()\n",
    "        if random_start:\n",
    "            adv_data = adv_data + torch.empty_like(adv_data).uniform_(-self.eps, self.eps)\n",
    "            adv_data = torch.clamp(adv_data, 0, 1).detach()\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            adv_data = adv_data + self.alpha * adv_data.grad.sign()\n",
    "            delta = torch.clamp(adv_data - data, min=-self.eps, max=self.eps)\n",
    "            adv_data = torch.clamp(data + delta, 0, 1).detach()\n",
    "        return adv_data"
   ],
   "id": "4c5210960216913d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:55.323874Z",
     "start_time": "2024-10-06T17:38:55.305035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, labels, train_size, val_size, test_size, random_state=42):\n",
    "    if train_size + val_size + test_size > 1.0:\n",
    "        raise ValueError(\"Toplam oran 1.0'dan büyük olamaz.\")\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(data, labels, train_size=train_size, random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=test_size / (test_size + val_size), random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, labels, train_size=0.28, val_size=0.04, test_size=0.09)"
   ],
   "id": "40df20d19385a9f9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:38:58.071330Z",
     "start_time": "2024-10-06T17:38:57.199859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=512, output_size=2):\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layer1 = GraPhyLayer(input_size, hidden_dim)\n",
    "        self.layer2 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer5 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "def initialize_model(input_size, hidden_dim=512, output_size=2, learning_rate=0.0001, loss_function=None):\n",
    "    \"\"\"\n",
    "    Modeli ve optimizasyon ayarlarını başlatır.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Giriş boyutu.\n",
    "        hidden_dim (int): Gizli katman boyutu.\n",
    "        output_size (int): Çıkış boyutu.\n",
    "        learning_rate (float): Öğrenme oranı.\n",
    "        loss_function (nn.Module): Kayıp fonksiyonu (varsayılan: nn.CrossEntropyLoss).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Model, optimizer ve loss function.\n",
    "    \"\"\"\n",
    "    model = GraPhyModel(input_size, hidden_dim, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    criterion = loss_function if loss_function else nn.CrossEntropyLoss()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "# Kullanım\n",
    "input_size = X_train.shape[1]\n",
    "model, optimizer, criterion = initialize_model(input_size, hidden_dim=256, learning_rate=0.001, loss_function=nn.CrossEntropyLoss())"
   ],
   "id": "e916fbeb764484d8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:39:09.337064Z",
     "start_time": "2024-10-06T17:39:01.086658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Modeli eğitmek için dinamik bir fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Eğitim yapılacak model.\n",
    "        train_loader (DataLoader): Eğitim veri yükleyicisi.\n",
    "        val_loader (DataLoader): Doğrulama veri yükleyicisi.\n",
    "        optimizer (torch.optim.Optimizer): Optimizasyon algoritması.\n",
    "        criterion (nn.Module): Kayıp fonksiyonu.\n",
    "        num_epochs (int): Eğitim epoch sayısı.\n",
    "        device (str): 'cpu' veya 'cuda' (GPU kullanımı için).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Eğitilmiş model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Eğitim modu\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Doğrulama modu\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Kullanım\n",
    "model = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50, device='cuda')"
   ],
   "id": "dc77e1ba79b33cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 12.2682, Validation Loss: 4.4748\n",
      "Epoch [2/50], Loss: 6.0266, Validation Loss: 4.3963\n",
      "Epoch [3/50], Loss: 5.5184, Validation Loss: 4.3354\n",
      "Epoch [4/50], Loss: 5.5581, Validation Loss: 4.2708\n",
      "Epoch [5/50], Loss: 5.3445, Validation Loss: 4.2451\n",
      "Epoch [6/50], Loss: 5.3636, Validation Loss: 4.2887\n",
      "Epoch [7/50], Loss: 5.2052, Validation Loss: 4.2075\n",
      "Epoch [8/50], Loss: 5.2350, Validation Loss: 4.1990\n",
      "Epoch [9/50], Loss: 5.3892, Validation Loss: 4.1421\n",
      "Epoch [10/50], Loss: 5.2214, Validation Loss: 4.1043\n",
      "Epoch [11/50], Loss: 5.1814, Validation Loss: 4.1141\n",
      "Epoch [12/50], Loss: 5.1718, Validation Loss: 4.1037\n",
      "Epoch [13/50], Loss: 5.1611, Validation Loss: 4.1176\n",
      "Epoch [14/50], Loss: 5.2026, Validation Loss: 4.1623\n",
      "Epoch [15/50], Loss: 5.1363, Validation Loss: 4.0717\n",
      "Epoch [16/50], Loss: 5.0201, Validation Loss: 4.0594\n",
      "Epoch [17/50], Loss: 5.1317, Validation Loss: 4.0794\n",
      "Epoch [18/50], Loss: 5.0604, Validation Loss: 4.0745\n",
      "Epoch [19/50], Loss: 5.1062, Validation Loss: 4.1288\n",
      "Epoch [20/50], Loss: 5.0869, Validation Loss: 4.1300\n",
      "Epoch [21/50], Loss: 5.1495, Validation Loss: 4.0701\n",
      "Epoch [22/50], Loss: 5.0598, Validation Loss: 4.0970\n",
      "Epoch [23/50], Loss: 4.9531, Validation Loss: 4.0676\n",
      "Epoch [24/50], Loss: 4.9620, Validation Loss: 4.0796\n",
      "Epoch [25/50], Loss: 5.0006, Validation Loss: 4.1212\n",
      "Epoch [26/50], Loss: 4.9860, Validation Loss: 4.1088\n",
      "Epoch [27/50], Loss: 4.9378, Validation Loss: 4.0918\n",
      "Epoch [28/50], Loss: 4.9782, Validation Loss: 4.2050\n",
      "Epoch [29/50], Loss: 4.9527, Validation Loss: 4.1077\n",
      "Epoch [30/50], Loss: 5.0371, Validation Loss: 4.1642\n",
      "Epoch [31/50], Loss: 4.9297, Validation Loss: 4.1354\n",
      "Epoch [32/50], Loss: 4.9677, Validation Loss: 4.1473\n",
      "Epoch [33/50], Loss: 4.8663, Validation Loss: 4.2289\n",
      "Epoch [34/50], Loss: 4.8612, Validation Loss: 4.1883\n",
      "Epoch [35/50], Loss: 5.0238, Validation Loss: 4.2541\n",
      "Epoch [36/50], Loss: 4.9348, Validation Loss: 4.2655\n",
      "Epoch [37/50], Loss: 4.9263, Validation Loss: 4.2051\n",
      "Epoch [38/50], Loss: 4.8629, Validation Loss: 4.2542\n",
      "Epoch [39/50], Loss: 4.7346, Validation Loss: 4.2352\n",
      "Epoch [40/50], Loss: 4.8133, Validation Loss: 4.2444\n",
      "Epoch [41/50], Loss: 4.7340, Validation Loss: 4.3522\n",
      "Epoch [42/50], Loss: 4.7529, Validation Loss: 4.3358\n",
      "Epoch [43/50], Loss: 4.8819, Validation Loss: 4.3905\n",
      "Epoch [44/50], Loss: 4.8636, Validation Loss: 4.4367\n",
      "Epoch [45/50], Loss: 4.6463, Validation Loss: 4.4245\n",
      "Epoch [46/50], Loss: 4.7224, Validation Loss: 4.4387\n",
      "Epoch [47/50], Loss: 4.5840, Validation Loss: 4.6326\n",
      "Epoch [48/50], Loss: 4.6791, Validation Loss: 4.5598\n",
      "Epoch [49/50], Loss: 4.6673, Validation Loss: 4.4985\n",
      "Epoch [50/50], Loss: 4.6957, Validation Loss: 4.5808\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:39:12.708215Z",
     "start_time": "2024-10-06T17:39:12.649684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Modeli test etmek için dinamik bir fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Test edilecek model.\n",
    "        test_loader (DataLoader): Test veri yükleyicisi.\n",
    "        device (str): 'cpu' veya 'cuda' (GPU kullanımı için).\n",
    "\n",
    "    Returns:\n",
    "        float: Test doğruluk oranı (%).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Kullanım\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader, device='cuda')"
   ],
   "id": "5307c07d22ee65aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.79%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T17:41:27.631729Z",
     "start_time": "2024-10-06T17:41:27.307343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model_with_metrics(model, test_loader, target_names, device='cpu'):\n",
    "    \"\"\"\n",
    "    Modeli test eder ve metrikleri hesaplar.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Test edilecek model.\n",
    "        test_loader (DataLoader): Test veri yükleyicisi.\n",
    "        target_names (list): Sınıf isimleri (ör. ['Normal', 'Anomali']).\n",
    "        device (str): 'cpu' veya 'cuda' (GPU kullanımı için).\n",
    "\n",
    "    Returns:\n",
    "        dict: Hesaplanan metrikler (accuracy, specificity, precision, recall, f1).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, target_names=target_names, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Kullanım\n",
    "metrics = evaluate_model_with_metrics(model, test_loader, target_names=['Normal', 'Anomali'], device='cuda')"
   ],
   "id": "9f4c682b46f37369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      1.00      0.97      1382\n",
      "     Anomali       0.00      0.00      0.00        76\n",
      "\n",
      "    accuracy                           0.95      1458\n",
      "   macro avg       0.47      0.50      0.49      1458\n",
      "weighted avg       0.90      0.95      0.92      1458\n",
      "\n",
      "Accuracy: 0.9479\n",
      "Specificity: 1.0000\n",
      "Precision: nan\n",
      "Recall: 0.0000\n",
      "F1 Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2135518426.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)    # Doğruluk (Precision)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AnomalyGenerator:\n",
    "    def __init__(self, model, eps=0.1, alpha=0.01, steps=10, random_start=True):\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.random_start = random_start\n",
    "        self.methods = {\n",
    "            'fgsm': self.fgsm,\n",
    "            'bim': self.bim,\n",
    "            'pgd': self.pgd\n",
    "        }\n",
    "\n",
    "    def fgsm(self, data, labels):\n",
    "        data = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        grad = torch.autograd.grad(loss, data)[0]\n",
    "        adv_data = data + self.eps * grad.sign()\n",
    "        return torch.clamp(adv_data, min=0, max=1)\n",
    "\n",
    "    def bim(self, data, labels):\n",
    "        adv_data = data.clone().detach()\n",
    "        for _ in range(self.steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            adv_data = adv_data + self.alpha * adv_data.grad.sign()\n",
    "            adv_data = torch.clamp(adv_data, min=0, max=1).detach()\n",
    "        return adv_data\n",
    "\n",
    "    def pgd(self, data, labels):\n",
    "        adv_data = data.clone().detach()\n",
    "        if self.random_start:\n",
    "            adv_data = adv_data + torch.empty_like(adv_data).uniform_(-self.eps, self.eps)\n",
    "            adv_data = torch.clamp(adv_data, min=0, max=1).detach()\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            adv_data.requires_grad = True\n",
    "            outputs = self.model(adv_data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            adv_data = adv_data + self.alpha * adv_data.grad.sign()\n",
    "            delta = torch.clamp(adv_data - data, min=-self.eps, max=self.eps)\n",
    "            adv_data = torch.clamp(data + delta, min=0, max=1).detach()\n",
    "        return adv_data\n",
    "\n",
    "    def generate_anomalies(self, data, labels, method='fgsm'):\n",
    "        if method in self.methods:\n",
    "            return self.methods[method](data, labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")"
   ],
   "id": "a3f0cb5ad98d16a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anomaly_gen = AnomalyGenerator(model)\n",
    "X_test_adv = anomaly_gen.generate_anomalies(X_test, y_test)"
   ],
   "id": "7d695d65fbaaa289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "814b5163cc14cf1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32bca0d31763aa7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab4a0c8519b3004e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:33.812624Z",
     "start_time": "2024-10-06T18:34:33.775598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_data(X, y, train_size=0.28, val_size=0.04, test_size=0.09, random_state=42):\n",
    "    \"\"\"\n",
    "    Veriyi eğitim, doğrulama ve test setlerine böler.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Özellikler.\n",
    "        y (np.ndarray): Etiketler.\n",
    "        train_size (float): Eğitim seti oranı.\n",
    "        val_size (float): Doğrulama seti oranı.\n",
    "        test_size (float): Test seti oranı.\n",
    "        random_state (int): Rastgelelik için başlangıç değeri.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Eğitim, doğrulama ve test setleri (X_train, X_val, X_test, y_train, y_val, y_test).\n",
    "    \"\"\"\n",
    "    if train_size + val_size + test_size > 1.0:\n",
    "        raise ValueError(\"Toplam oran 1.0'dan büyük olamaz.\")\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, random_state=random_state)\n",
    "    val_test_ratio = test_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_test_ratio, random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Kullanım\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size=0.28, val_size=0.04, test_size=0.09)"
   ],
   "id": "83ea84922a63b00d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:35.856854Z",
     "start_time": "2024-10-06T18:34:35.851098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, output_size):\n",
    "        \"\"\"\n",
    "        Dinamik GraPhyModel sınıfı.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Giriş boyutu.\n",
    "            hidden_dims (list): Gizli katman boyutlarını içeren liste.\n",
    "            output_size (int): Çıkış boyutu.\n",
    "        \"\"\"\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # İlk katman\n",
    "        self.layers.append(GraPhyLayer(input_size, hidden_dims[0]))\n",
    "\n",
    "        # Ara katmanlar\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.layers.append(GraPhyLayer(hidden_dims[i - 1], hidden_dims[i]))\n",
    "\n",
    "        # Çıkış katmanı\n",
    "        self.fc_out = nn.Linear(hidden_dims[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ],
   "id": "ebf36b8ecde56a87",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:34:36.879673Z",
     "start_time": "2024-10-06T18:34:36.862700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnomalyGenerator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.methods = {\n",
    "            'fgsm': self.generate_fgsm,\n",
    "            'bim': self.generate_bim,\n",
    "            'pgd': self.generate_pgd\n",
    "        }\n",
    "\n",
    "    def generate_fgsm(self, data, labels, eps=0.1):\n",
    "        data = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        adv_data = data + eps * data.grad.sign()\n",
    "        return adv_data.detach()\n",
    "\n",
    "    def generate_bim(self, data, labels, eps=0.1, alpha=0.01, steps=10):\n",
    "        data = data.clone().detach()\n",
    "        ori_data = data.clone().detach()\n",
    "\n",
    "        for _ in range(steps):\n",
    "            data.requires_grad = True\n",
    "            outputs = self.model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            data = data + alpha * data.grad.sign()\n",
    "            data = torch.clamp(data, ori_data - eps, ori_data + eps).detach()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def generate_pgd(self, data, labels, eps=0.1, alpha=0.01, steps=10, random_start=True):\n",
    "        data = data.clone().detach()\n",
    "        if random_start:\n",
    "            data = data + torch.empty_like(data).uniform_(-eps, eps)\n",
    "            data = torch.clamp(data, min=0, max=1).detach()\n",
    "\n",
    "        ori_data = data.clone().detach()\n",
    "\n",
    "        for _ in range(steps):\n",
    "            data.requires_grad = True\n",
    "            outputs = self.model(data)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            data = data + alpha * data.grad.sign()\n",
    "            data = torch.clamp(data, ori_data - eps, ori_data + eps).detach()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def generate_anomalies(self, data, labels, method='fgsm', **kwargs):\n",
    "        \"\"\"\n",
    "        Dinamik olarak belirtilen yönteme göre anomali üretir.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor): Giriş verisi.\n",
    "            labels (torch.Tensor): Etiketler.\n",
    "            method (str): Kullanılacak yöntem ('fgsm', 'bim', 'pgd').\n",
    "            **kwargs: Yönteme özel parametreler.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Üretilen anomali verisi.\n",
    "        \"\"\"\n",
    "        if method in self.methods:\n",
    "            return self.methods[method](data, labels, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")"
   ],
   "id": "1cdd43a8a6a2de76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:35:05.185383Z",
     "start_time": "2024-10-06T18:34:37.661164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Modeli eğitmek için dinamik bir fonksiyon.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Eğitim yapılacak model.\n",
    "        train_loader (DataLoader): Eğitim veri yükleyicisi.\n",
    "        val_loader (DataLoader): Doğrulama veri yükleyicisi.\n",
    "        optimizer (torch.optim.Optimizer): Optimizasyon algoritması.\n",
    "        criterion (nn.Module): Kayıp fonksiyonu.\n",
    "        num_epochs (int): Eğitim epoch sayısı.\n",
    "        device (str): 'cpu' veya 'cuda' (GPU kullanımı için).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Eğitilmiş model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Eğitim modu\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Doğrulama modu\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = GraPhyModel(input_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50, device='cuda')"
   ],
   "id": "226a3b8d48086846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 24.1026, Validation Loss: 1.5193\n",
      "Epoch [2/50], Loss: 15.6634, Validation Loss: 1.3401\n",
      "Epoch [3/50], Loss: 15.5176, Validation Loss: 1.4785\n",
      "Epoch [4/50], Loss: 15.0240, Validation Loss: 1.3777\n",
      "Epoch [5/50], Loss: 14.8722, Validation Loss: 1.3524\n",
      "Epoch [6/50], Loss: 14.8079, Validation Loss: 1.2901\n",
      "Epoch [7/50], Loss: 14.6921, Validation Loss: 1.3838\n",
      "Epoch [8/50], Loss: 14.7178, Validation Loss: 1.4183\n",
      "Epoch [9/50], Loss: 14.8644, Validation Loss: 1.3729\n",
      "Epoch [10/50], Loss: 14.6545, Validation Loss: 1.3491\n",
      "Epoch [11/50], Loss: 14.7580, Validation Loss: 1.3560\n",
      "Epoch [12/50], Loss: 14.6857, Validation Loss: 1.4932\n",
      "Epoch [13/50], Loss: 14.6414, Validation Loss: 1.4618\n",
      "Epoch [14/50], Loss: 14.5706, Validation Loss: 1.4367\n",
      "Epoch [15/50], Loss: 14.4608, Validation Loss: 1.3983\n",
      "Epoch [16/50], Loss: 14.4717, Validation Loss: 1.4505\n",
      "Epoch [17/50], Loss: 14.5682, Validation Loss: 1.4234\n",
      "Epoch [18/50], Loss: 14.3284, Validation Loss: 1.5200\n",
      "Epoch [19/50], Loss: 14.4042, Validation Loss: 1.4461\n",
      "Epoch [20/50], Loss: 14.4444, Validation Loss: 1.4023\n",
      "Epoch [21/50], Loss: 14.3007, Validation Loss: 1.4395\n",
      "Epoch [22/50], Loss: 14.2683, Validation Loss: 1.4379\n",
      "Epoch [23/50], Loss: 14.2020, Validation Loss: 1.4636\n",
      "Epoch [24/50], Loss: 14.4028, Validation Loss: 1.3989\n",
      "Epoch [25/50], Loss: 14.2638, Validation Loss: 1.4179\n",
      "Epoch [26/50], Loss: 14.2891, Validation Loss: 1.5786\n",
      "Epoch [27/50], Loss: 14.0325, Validation Loss: 1.6681\n",
      "Epoch [28/50], Loss: 14.2216, Validation Loss: 1.4331\n",
      "Epoch [29/50], Loss: 14.4053, Validation Loss: 1.4962\n",
      "Epoch [30/50], Loss: 14.3386, Validation Loss: 1.3423\n",
      "Epoch [31/50], Loss: 14.1795, Validation Loss: 1.4994\n",
      "Epoch [32/50], Loss: 14.2345, Validation Loss: 1.4586\n",
      "Epoch [33/50], Loss: 13.9529, Validation Loss: 1.5145\n",
      "Epoch [34/50], Loss: 14.1309, Validation Loss: 1.5285\n",
      "Epoch [35/50], Loss: 14.1108, Validation Loss: 1.4103\n",
      "Epoch [36/50], Loss: 14.2714, Validation Loss: 1.5869\n",
      "Epoch [37/50], Loss: 14.1025, Validation Loss: 1.3617\n",
      "Epoch [38/50], Loss: 13.9786, Validation Loss: 1.6674\n",
      "Epoch [39/50], Loss: 14.1396, Validation Loss: 1.7348\n",
      "Epoch [40/50], Loss: 14.4306, Validation Loss: 1.4937\n",
      "Epoch [41/50], Loss: 14.0409, Validation Loss: 1.4048\n",
      "Epoch [42/50], Loss: 13.9067, Validation Loss: 1.5212\n",
      "Epoch [43/50], Loss: 14.0531, Validation Loss: 1.4908\n",
      "Epoch [44/50], Loss: 13.9232, Validation Loss: 1.4305\n",
      "Epoch [45/50], Loss: 13.9699, Validation Loss: 1.4469\n",
      "Epoch [46/50], Loss: 13.9293, Validation Loss: 1.4569\n",
      "Epoch [47/50], Loss: 14.1389, Validation Loss: 1.4516\n",
      "Epoch [48/50], Loss: 14.0217, Validation Loss: 1.4855\n",
      "Epoch [49/50], Loss: 13.8455, Validation Loss: 1.6031\n",
      "Epoch [50/50], Loss: 13.7397, Validation Loss: 1.5136\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:35:10.908828Z",
     "start_time": "2024-10-06T18:35:08.975051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_anomalies(model, X_test, y_test, anomaly_generator, methods=['fgsm', 'bim', 'pgd'], device='cpu'):\n",
    "    \"\"\"\n",
    "    Modeli anomali verileriyle test eder ve metrikleri hesaplar.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Test edilecek model.\n",
    "        X_test (np.ndarray): Test verisi.\n",
    "        y_test (np.ndarray): Test etiketleri.\n",
    "        anomaly_generator (AnomalyGenerator): Anomali üretici.\n",
    "        methods (list): Kullanılacak anomali üretim yöntemleri.\n",
    "        device (str): 'cpu' veya 'cuda' (GPU kullanımı için).\n",
    "\n",
    "    Returns:\n",
    "        dict: Hesaplanan metrikler (accuracy, precision, recall, specificity, f1).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Anomali verilerini üret\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "    all_anomalies = []\n",
    "    for method in methods:\n",
    "        anomalies = anomaly_generator.generate_anomalies(X_test_tensor, y_test_tensor, method=method)\n",
    "        all_anomalies.append(anomalies.cpu().numpy())\n",
    "\n",
    "    # Anomali verilerini birleştir\n",
    "    X_test_with_anomalies = np.vstack([X_test] + all_anomalies)\n",
    "    y_test_with_anomalies = np.hstack([y_test, np.ones(sum(a.shape[0] for a in all_anomalies))])\n",
    "\n",
    "    # Tensorlara dönüştür\n",
    "    X_test_tensor = torch.FloatTensor(X_test_with_anomalies).to(device)\n",
    "    y_test_tensor = torch.LongTensor(y_test_with_anomalies).to(device)\n",
    "\n",
    "    # Modeli test et\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "    # Metrikleri hesapla\n",
    "    accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "    precision = precision_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "    recall = recall_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_tensor.cpu().numpy(), predicted.cpu().numpy()).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    # Sonuçları yazdır\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Kullanım\n",
    "metrics = evaluate_with_anomalies(model, X_test, y_test, anomaly_generator, methods=['fgsm', 'bim', 'pgd'], device='cuda')"
   ],
   "id": "7708e4cacc0f25ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0310\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Specificity: 1.0000\n",
      "F1 Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/var/folders/vr/11d0bknx2ms1fxq4p99sr8tc0000gn/T/ipykernel_11112/2242473391.py:33: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f1 = 2 * (precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:07:56.884565Z",
     "start_time": "2024-10-06T19:07:56.280767Z"
    }
   },
   "cell_type": "code",
   "source": "merged_data.info()",
   "id": "fdb960878cb7019",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2924 entries, 0 to 2923\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   timestamp    2924 non-null   datetime64[ns, UTC]\n",
      " 1   pm25_avg_60  2924 non-null   float64            \n",
      " 2   windspeed    2924 non-null   float64            \n",
      " 3   winddir      2924 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(3)\n",
      "memory usage: 178.8 KB\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:38:57.948572Z",
     "start_time": "2024-10-06T18:38:57.917683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# merged_data = pd.read_csv('your_data.csv') # Veriyi yükleyin\n",
    "\n",
    "def normalise(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "features = ['pm25_avg_60', 'windspeed', 'winddir']\n",
    "merged_data[features] = normalise(merged_data[features])"
   ],
   "id": "da745c54ae8fa3aa",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:38:58.738584Z",
     "start_time": "2024-10-06T18:38:58.717196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_anomalies(data, num_anomalies=1000):\n",
    "    anomalies = []\n",
    "    for _ in range(num_anomalies):\n",
    "        index = np.random.randint(0, data.shape[0])\n",
    "        anomaly = data[index] + np.random.normal(0, 1, data[index].shape)\n",
    "        anomalies.append(anomaly)\n",
    "    return np.array(anomalies)\n",
    "\n",
    "normal_data = merged_data[features].values\n",
    "\n",
    "anomaly_data = generate_anomalies(normal_data, num_anomalies=1000)  # 1000 anomali üret"
   ],
   "id": "39ae7150ac0ec6c4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:39:43.067343Z",
     "start_time": "2024-10-06T18:39:43.045990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.vstack((normal_data, anomaly_data))\n",
    "y = np.hstack((np.zeros(normal_data.shape[0]), np.ones(anomaly_data.shape[0])))\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.13, random_state=42)  # %28 eğitim\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6923, random_state=42)  # %4 doğrulama ve %9 test"
   ],
   "id": "315ec490fc274f46",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:39:47.641090Z",
     "start_time": "2024-10-06T18:39:47.632525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraPhyLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraPhyLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class GraPhyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=512, output_size=2):\n",
    "        super(GraPhyModel, self).__init__()\n",
    "        self.layer1 = GraPhyLayer(input_size, hidden_dim)\n",
    "        self.layer2 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.layer5 = GraPhyLayer(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ],
   "id": "ebe56545f55b7765",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:40:15.936946Z",
     "start_time": "2024-10-06T18:39:51.707705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = GraPhyModel(input_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ],
   "id": "671b45462bc52ff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 60.1515, Validation Loss: 2.5585\n",
      "Epoch [2/50], Loss: 44.4334, Validation Loss: 2.1748\n",
      "Epoch [3/50], Loss: 37.2413, Validation Loss: 1.8297\n",
      "Epoch [4/50], Loss: 34.1474, Validation Loss: 1.7889\n",
      "Epoch [5/50], Loss: 31.9695, Validation Loss: 1.6694\n",
      "Epoch [6/50], Loss: 30.1265, Validation Loss: 1.6336\n",
      "Epoch [7/50], Loss: 29.5877, Validation Loss: 1.5462\n",
      "Epoch [8/50], Loss: 29.4214, Validation Loss: 1.4780\n",
      "Epoch [9/50], Loss: 28.1360, Validation Loss: 1.6396\n",
      "Epoch [10/50], Loss: 28.2332, Validation Loss: 1.5894\n",
      "Epoch [11/50], Loss: 27.0595, Validation Loss: 1.4489\n",
      "Epoch [12/50], Loss: 26.7501, Validation Loss: 1.5674\n",
      "Epoch [13/50], Loss: 26.2023, Validation Loss: 1.5061\n",
      "Epoch [14/50], Loss: 26.1530, Validation Loss: 1.5522\n",
      "Epoch [15/50], Loss: 25.7509, Validation Loss: 1.5736\n",
      "Epoch [16/50], Loss: 26.2591, Validation Loss: 1.4641\n",
      "Epoch [17/50], Loss: 25.2213, Validation Loss: 1.4762\n",
      "Epoch [18/50], Loss: 24.6848, Validation Loss: 1.4933\n",
      "Epoch [19/50], Loss: 24.2464, Validation Loss: 1.4335\n",
      "Epoch [20/50], Loss: 24.3083, Validation Loss: 1.4593\n",
      "Epoch [21/50], Loss: 24.5605, Validation Loss: 1.4317\n",
      "Epoch [22/50], Loss: 23.3696, Validation Loss: 1.4468\n",
      "Epoch [23/50], Loss: 23.6004, Validation Loss: 1.5441\n",
      "Epoch [24/50], Loss: 23.3293, Validation Loss: 1.4660\n",
      "Epoch [25/50], Loss: 22.8940, Validation Loss: 1.4556\n",
      "Epoch [26/50], Loss: 23.0645, Validation Loss: 1.5473\n",
      "Epoch [27/50], Loss: 23.0796, Validation Loss: 1.4333\n",
      "Epoch [28/50], Loss: 22.0331, Validation Loss: 1.4484\n",
      "Epoch [29/50], Loss: 22.2087, Validation Loss: 1.3129\n",
      "Epoch [30/50], Loss: 21.5647, Validation Loss: 1.4631\n",
      "Epoch [31/50], Loss: 21.6691, Validation Loss: 1.4889\n",
      "Epoch [32/50], Loss: 22.0339, Validation Loss: 1.5157\n",
      "Epoch [33/50], Loss: 21.0057, Validation Loss: 1.3381\n",
      "Epoch [34/50], Loss: 20.3418, Validation Loss: 1.4100\n",
      "Epoch [35/50], Loss: 21.3015, Validation Loss: 1.2924\n",
      "Epoch [36/50], Loss: 20.4548, Validation Loss: 1.5256\n",
      "Epoch [37/50], Loss: 20.2126, Validation Loss: 1.4036\n",
      "Epoch [38/50], Loss: 19.8283, Validation Loss: 1.3927\n",
      "Epoch [39/50], Loss: 19.5497, Validation Loss: 1.4366\n",
      "Epoch [40/50], Loss: 20.7876, Validation Loss: 1.3551\n",
      "Epoch [41/50], Loss: 19.8916, Validation Loss: 1.3751\n",
      "Epoch [42/50], Loss: 19.7614, Validation Loss: 1.3934\n",
      "Epoch [43/50], Loss: 21.0793, Validation Loss: 1.4120\n",
      "Epoch [44/50], Loss: 19.5036, Validation Loss: 1.3156\n",
      "Epoch [45/50], Loss: 19.7882, Validation Loss: 1.4077\n",
      "Epoch [46/50], Loss: 19.8729, Validation Loss: 1.4357\n",
      "Epoch [47/50], Loss: 18.7656, Validation Loss: 1.3462\n",
      "Epoch [48/50], Loss: 19.4130, Validation Loss: 1.4644\n",
      "Epoch [49/50], Loss: 21.4581, Validation Loss: 1.3262\n",
      "Epoch [50/50], Loss: 18.9836, Validation Loss: 1.3996\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:40:19.463004Z",
     "start_time": "2024-10-06T18:40:19.449657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.FloatTensor(X_test))\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "precision = precision_score(y_test, predicted.numpy())\n",
    "recall = recall_score(y_test, predicted.numpy())\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted.numpy()).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Specificity: {specificity:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ],
   "id": "3f8ec538f8f7250f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8898\n",
      "Precision: 0.8684\n",
      "Recall: 0.6947\n",
      "Specificity: 0.9614\n",
      "F1 Score: 0.7719\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:41:42.629705Z",
     "start_time": "2024-10-06T18:41:42.598376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test_outputs = model(test_inputs)\n",
    "\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "total_anomalies = np.sum(y_test)  \n",
    "correct_predictions = np.sum(predicted.numpy()[y_test == 1])  # Modelin doğru tahmin ettiği anomaliler\n",
    "\n",
    "print(f\"Toplam gerçek anomali sayısı: {total_anomalies}\")\n",
    "print(f\"Modelin doğru tahmin ettiği anomali sayısı: {correct_predictions}\")\n",
    "print(f\"Modelin doğruluk oranı: {correct_predictions / total_anomalies * 100:.2f}%\")"
   ],
   "id": "1c246e3798f4bcea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam gerçek anomali sayısı: 95.0\n",
      "Modelin doğru tahmin ettiği anomali sayısı: 66\n",
      "Modelin doğruluk oranı: 69.47%\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66b69555309c0e54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfacf7241a7c6f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d889fc5ba5fcf690"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
